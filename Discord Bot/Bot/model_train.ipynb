{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tflearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALdmucjVTkJQ",
        "outputId": "2e85ff01-757d-45bb-cb51-431d4e7a19a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (8.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEJO6szJTOae",
        "outputId": "43d7c48a-384b-49a0-e6c6-7d6a1de50289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "import numpy\n",
        "import tflearn\n",
        "import tensorflow\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import logging\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w450GO9Tod2",
        "outputId": "7f1a81dd-b8b6-4e58-db5d-3a462f6947a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('drive/MyDrive')"
      ],
      "metadata": {
        "id": "pGqkf-PcT8JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = os.getcwd()\n",
        "model_pth = os.path.join(BASE_DIR, \"model\")\n",
        "model_pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "u2zcBTTFUKZ8",
        "outputId": "89020bd3-9608-43ca-8c38-363b37bab28a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S')\n",
        "logger = logging.getLogger(\"ChatBot\")"
      ],
      "metadata": {
        "id": "0N-7ukQ3Ucwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aHwJJNmWNlL",
        "outputId": "a2402ffd-c7e9-4b8a-ed0e-b6bf9de93bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatBot(object):\n",
        "    data: dict\n",
        "    words: list\n",
        "    labels: list\n",
        "    training: list\n",
        "    output: list\n",
        "\n",
        "    @classmethod\n",
        "    def train(cls, epoch: int = 5000) -> None:\n",
        "        try:\n",
        "            with open(os.path.join(model_pth, \"intents.json\")) as file:\n",
        "                data = json.load(file)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(\"Cannot Find intents.json!\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            with open(os.path.join(model_pth, \"data.pickle\"), \"rb\") as f:\n",
        "                words, labels, training, output = pickle.load(f)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.info(\"Optimizing Data..\")\n",
        "            words = []\n",
        "            labels = []\n",
        "            docs_x = []\n",
        "            docs_y = []\n",
        "\n",
        "            for intent in data[\"intents\"]:\n",
        "                for pattern in intent[\"patterns\"]:\n",
        "                    wrds = nltk.word_tokenize(pattern)\n",
        "                    words.extend(wrds)\n",
        "                    docs_x.append(wrds)\n",
        "                    docs_y.append(intent[\"tag\"])\n",
        "\n",
        "                if intent[\"tag\"] not in labels:\n",
        "                    labels.append(intent[\"tag\"])\n",
        "\n",
        "            words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "            words = sorted(list(set(words)))\n",
        "\n",
        "            labels = sorted(labels)\n",
        "\n",
        "            training = []\n",
        "            output = []\n",
        "\n",
        "            out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "            for x, doc in enumerate(docs_x):\n",
        "                bag = []\n",
        "\n",
        "                wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "\n",
        "                for w in words:\n",
        "                    if w in wrds:\n",
        "                        bag.append(1)\n",
        "                    else:\n",
        "                        bag.append(0)\n",
        "\n",
        "                output_row = out_empty[:]\n",
        "                output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "                training.append(bag)\n",
        "                output.append(output_row)\n",
        "\n",
        "            training = numpy.array(training)\n",
        "            output = numpy.array(output)\n",
        "\n",
        "            with open(os.path.join(model_pth, \"data.pickle\"), \"wb\") as f:\n",
        "                pickle.dump((words, labels, training, output), f)\n",
        "\n",
        "            logger.info(\"Saved Optimized Data\")\n",
        "\n",
        "        tensorflow.compat.v1.reset_default_graph()\n",
        "        net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "        net = tflearn.fully_connected(net, 8)\n",
        "        net = tflearn.fully_connected(net, 8)\n",
        "        net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
        "        net = tflearn.regression(net)\n",
        "        model = tflearn.DNN(net)\n",
        "        model.fit(training, output, n_epoch=epoch, batch_size=8, show_metric=True)\n",
        "        model.save(os.path.join(model_pth, \"model.tflearn\"))\n",
        "\n",
        "    @classmethod\n",
        "    def model(cls):\n",
        "        try:\n",
        "            with open(os.path.join(model_pth, \"data.pickle\"), \"rb\") as f:\n",
        "                words, labels, training, output = pickle.load(f)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(\"Cannot Find Data! Please Make Sure You Have Optimized Your Data First!\")\n",
        "            quit()\n",
        "\n",
        "        tensorflow.compat.v1.reset_default_graph()\n",
        "        net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "        net = tflearn.fully_connected(net, 8)\n",
        "        net = tflearn.fully_connected(net, 8)\n",
        "        net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
        "        net = tflearn.regression(net)\n",
        "        model = tflearn.DNN(net)\n",
        "\n",
        "        try:\n",
        "            model.load(os.path.join(model_pth, \"model.tflearn\"))\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(\"Cannot Find Model! Please Make Sure You Have Trained A Model!\")\n",
        "            quit()\n",
        "\n",
        "    @classmethod\n",
        "    def to_bag_of_words(cls, s: str, words: str) -> numpy.array:\n",
        "        bag = [0 for _ in range(len(words))]\n",
        "        s_words = nltk.word_tokenize(s)\n",
        "        s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "        for se in s_words:\n",
        "            for i, w in enumerate(words):\n",
        "                if w == se:\n",
        "                    bag[i] = 1\n",
        "\n",
        "        return numpy.array(bag)\n",
        "\n",
        "    @classmethod\n",
        "    def chat(cls, text) -> None:\n",
        "        model = cls.model()\n",
        "        try:\n",
        "            with open(os.path.join(model_pth, \"intents.json\")) as file:\n",
        "                data = json.load(file)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(\"Cannot Find intents.json!\")\n",
        "            quit()\n",
        "\n",
        "        try:\n",
        "            with open(os.path.join(model_pth, \"data.pickle\"), \"rb\") as f:\n",
        "                words, labels, training, output = pickle.load(f)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(\"Cannot Find Data! Please Make Sure That You Have Optimized Your Data First!\")\n",
        "            quit()\n",
        "\n",
        "        results = model.predict([cls.to_bag_of_words(text, words)])\n",
        "        results_index = numpy.argmax(results)\n",
        "        tag = labels[results_index]\n",
        "\n",
        "        for tg in data[\"intents\"]:\n",
        "            if tg['tag'] == tag:\n",
        "\n",
        "                responses = tg['responses']\n",
        "                return random.choice(responses)\n",
        "\n",
        "\n",
        "            else:\n",
        "                 random.choice([\"I dont have a reply for that\",\n",
        "                                  \"I am just a prototype I am not finished yet. So I cant answer that for now\",\n",
        "                                  \"Sorry I dont have a reply for you now\"])"
      ],
      "metadata": {
        "id": "APnkfeyvUs1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_bot = ChatBot()"
      ],
      "metadata": {
        "id": "4qTj_WscU5Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_bot.train(5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zeo1V8cU-DX",
        "outputId": "55af0a17-eb74-4f3b-ed61-c800270a5c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 19999  | total loss: \u001b[1m\u001b[32m0.12756\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 5000 | loss: 0.12756 - acc: 0.9940 -- iter: 24/32\n",
            "Training Step: 20000  | total loss: \u001b[1m\u001b[32m0.12317\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 5000 | loss: 0.12317 - acc: 0.9946 -- iter: 32/32\n",
            "--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JISDkk9qbqBa",
        "outputId": "f77facff-4dee-467b-d264-46fe8b19131b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['intents.json',\n",
              " 'data.pickle',\n",
              " 'model.tflearn.data-00000-of-00001',\n",
              " 'model.tflearn.index',\n",
              " 'checkpoint',\n",
              " 'model.tflearn.meta']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}